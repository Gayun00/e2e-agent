# Playwright E2E Agent 설계 과정

## 프로젝트 개요

Playwright E2E 테스트를 자동으로 생성하는 AI Agent 개발 프로젝트입니다. Page Object Model(POM) 패턴을 따르며, 사용자가 제공한 시나리오와 도메인 지식을 기반으로 테스트 코드를 생성합니다.

## 설계 과정

### 1단계: 초기 아이디어 및 요구사항 정의

**초기 아이디어:**
- POM 패턴 기반 테스트 자동 생성
- 시나리오 분석 → 페이지 객체 생성 → 요소 선택자 결정 → 테스트 코드 생성
- LLM이 요소 선택자를 추측하고 개발자 확인 받기

**핵심 워크플로우:**
1. 시나리오에 필요한 페이지 객체 생성 (POM)
2. 상호작용할 요소 선택 방식 결정
3. 페이지별 동작 프로퍼티 생성
4. 시나리오별 테스트 작성
5. E2E 테스트 문서 작성 및 업데이트

### 2단계: 요구사항 구체화

**주요 결정 사항:**

#### 2.1 도메인 지식 문서의 필요성
**문제:** Agent가 테스트를 작성하려면 애플리케이션에 대한 컨텍스트가 필요
**해결:** 도메인 지식 문서 도입
- 페이지 정보 (경로, 설명, 접근 방법)
- 테스트 플로우
- 도메인 특화 정보 (인증, 데이터, 로딩 등)

**참고 시점:** 페이지 객체 생성 전에 먼저 참고하여 정확도 향상

#### 2.2 페이지 경로 확인 프로세스
**문제:** LLM이 추론한 경로가 정확하지 않을 수 있음
**해결:** 단계별 확인 프로세스
1. 도메인 문서에서 경로 확인
2. 없으면 LLM이 추론
3. 사용자에게 확인 요청
4. 확정된 경로로 `goto()` 메서드 및 검증 메서드 생성

#### 2.3 요소 선택자 검증 프로세스
**문제:** 선택자가 실제로 작동하는지 확인 필요
**해결:** 브라우저 기반 검증
1. 브라우저 실행
2. 여러 선택자 전략 분석 (role, placeholder, test-id 등)
3. 실제 페이지에서 검증
4. 사용자에게 확인 및 수정 요청

#### 2.4 CLI 인터페이스
**문제:** 사용자가 쉽게 접근하고 자동화 파이프라인에 통합 필요
**해결:** CLI 도구 제공
- 대화형 모드
- 설정 파일 지원
- 사용자 지정 디렉토리 또는 Playwright 기본 구조 사용

### 3단계: 기술 스택 선정

#### 3.1 브라우저 자동화: MCP 통합 결정

**초기 계획:** Playwright를 직접 사용하여 브라우저 제어
**문제점:** 브라우저 제어 로직을 처음부터 구현해야 함

**개선안:** Microsoft Playwright MCP 서버 활용
**이유:**
- 이미 검증된 도구들 제공
- 브라우저 제어 로직 구현 불필요
- 선택자 검증, 스크린샷, 요소 상호작용 등 기능 즉시 사용 가능

**MCP 도구 활용:**
- `playwright_navigate`: 페이지 이동 및 경로 검증
- `playwright_click/fill`: 선택자 검증
- `playwright_screenshot`: 시각적 피드백
- `playwright_evaluate`: 페이지 구조 분석
- `playwright_get_text/get_attribute`: 요소 정보 확인

**아키텍처 변경:**
```
Agent → MCP Client → Playwright MCP Server → Browser
```

#### 3.2 LLM 선택: Claude vs GPT-4

**고려 사항:**
1. 여러 LLM 지원 가능성
2. Prompt Caching 필요성
3. 코드 생성 품질
4. 비용 효율성

**Prompt Caching의 중요성:**
- 시스템 프롬프트 (2,000 토큰)
- 도메인 지식 (3,000 토큰)
- 기존 페이지 객체 (5,000 토큰)
- **총 10,000 토큰이 매 호출마다 반복**

**비용 비교:**
- 캐싱 없이: 10,000 토큰 × 20회 = 200,000 토큰
- 캐싱 사용: 10,000 + (1,000 × 19회) = 29,000 토큰 (85% 절감)

**SDK 선택 논의:**

**옵션 1: Vercel AI SDK**
- ✅ 여러 LLM 지원 (Claude, GPT-4, Gemini)
- ✅ Langfuse 통합
- ✅ 구조화된 출력
- ❌ Anthropic Prompt Caching 미지원

**옵션 2: Anthropic SDK**
- ✅ Prompt Caching 지원
- ✅ Claude에 최적화
- ✅ 긴 컨텍스트 (200K)
- ❌ 초기에는 Claude만 지원

**최종 결정: Anthropic SDK + Claude 3.5 Sonnet**

**이유:**
1. **코드 생성 품질**: TypeScript/Playwright 코드 생성 우수
2. **긴 컨텍스트**: 200K 토큰 - 모든 컨텍스트 포함 가능
3. **Prompt Caching**: 비용 90% 절감
4. **구조화된 추론**: 단계별 분석 및 코드 생성 탁월
5. **비용 효율성**: GPT-4 대비 저렴

**향후 확장:** 추상화 레이어를 통해 다른 LLM 지원 가능

#### 3.4 스크린샷 및 Mocking 기능

**필요성:**
- 페이지의 다양한 상태를 시각적으로 문서화
- API 응답이나 스토리지 값에 따라 달라지는 UI 캡처
- PC/Mobile 등 다양한 디바이스별 화면 확인

**설계 결정:**

**Mocking 방식: 페이지별 설정 (POM에 포함)**
- 전역 Mocking 대신 페이지별로 필요한 mocking 정의
- 재사용 가능하고 시나리오별로 파라미터화
- `setupMocks(scenario)` 메서드로 POM에 추가

**워크플로우:**
1. 페이지 코드 분석 → API/스토리지 의존성 파악
2. 기존 mocking 설정 확인
3. 없으면 LLM이 생성 → 사용자 확인
4. POM에 `setupMocks()` 메서드 추가
5. Mocking 적용하여 PC/Mobile 스크린샷 생성
6. `screenshots/PageName/` 디렉토리에 저장

**예시:**
```typescript
// LoginPage.ts
async setupMocks(scenario: 'success' | 'error' = 'success') {
  await this.page.route('/api/auth/login', route => {
    route.fulfill({
      status: scenario === 'error' ? 401 : 200,
      body: JSON.stringify({ token: 'mock-token' })
    });
  });
  
  await this.page.evaluate(() => {
    localStorage.setItem('theme', 'dark');
  });
}
```

**장점:**
- 재사용 가능: 테스트에서도 동일한 mocking 사용
- 유연성: 시나리오별로 다른 응답 설정 가능
- 문서화: 페이지가 어떤 데이터에 의존하는지 명확

#### 3.3 모니터링: Langfuse

**선택 이유:**
- LLM 호출 추적 및 분석
- 비용 모니터링
- 성능 최적화
- 디버깅 지원

### 4단계: 아키텍처 설계

#### 레이어 구조

```
┌─────────────────────────────────────────────────────────────┐
│                         CLI Layer                            │
│              (사용자 인터페이스 및 입력 처리)                │
└───────────────────────────┬─────────────────────────────────┘
                            │
┌─────────────────────────────────────────────────────────────┐
│                    Orchestration Layer                       │
│                  (워크플로우 관리 및 조율)                   │
└───────────────────────────┬─────────────────────────────────┘
                            │
┌─────────────────────────────────────────────────────────────┐
│                      Service Layer                           │
│     (도메인 문서, 페이지 객체, 선택자, 테스트 구성)         │
└───────────────────────────┬─────────────────────────────────┘
                            │
┌─────────────────────────────────────────────────────────────┐
│                        LLM Layer                             │
│              (Anthropic API + Prompt Caching)                │
└───────────────────────────┬─────────────────────────────────┘
                            │
┌─────────────────────────────────────────────────────────────┐
│                   Infrastructure Layer                       │
│            (Langfuse, MCP Client, File System)               │
└─────────────────────────────────────────────────────────────┘
```

#### 핵심 컴포넌트

1. **Agent Orchestrator**: 워크플로우 관리
2. **Domain Document Manager**: 도메인 지식 관리
3. **Page Object Generator**: 페이지 객체 생성
4. **MCP Client Service**: Playwright MCP 서버 통신
5. **Selector Determiner**: 선택자 결정 및 검증
6. **Screenshot and Mocking Service**: 스크린샷 생성 및 API/스토리지 Mocking
7. **Test Scenario Composer**: 테스트 시나리오 구성
8. **LLM Service Layer**: Claude API 호출 및 캐싱

### 5단계: 워크플로우 정의

```
1. 도메인 지식 로드
   ↓
2. 테스트 시나리오 분석
   ↓
3. 페이지 객체 생성
   - 페이지 식별
   - 경로 추론 및 확인
   - goto() 및 검증 메서드 생성
   ↓
4. 요소 선택자 결정
   - MCP로 브라우저 실행
   - 선택자 후보 생성
   - 실제 페이지에서 검증
   - 사용자 확인
   ↓
5. Mocking 설정 및 스크린샷 생성
   - 페이지 의존성 분석 (API, 스토리지)
   - 기존 mocking 설정 확인
   - 없으면 LLM으로 생성 → 사용자 확인
   - POM에 setupMocks() 메서드 추가
   - PC/Mobile 스크린샷 생성
   ↓
6. 테스트 동작 생성
   - 페이지별 액션 메서드
   - 적절한 대기 및 검증 포함
   ↓
7. 테스트 시나리오 구성
   - describe/test 블록 생성
   - setup/teardown 추가
   ↓
8. 문서 업데이트
   - 도메인 지식 문서 갱신
```

## 주요 설계 결정 요약

| 항목 | 결정 | 이유 |
|------|------|------|
| 브라우저 자동화 | MCP 통합 | 검증된 도구 활용, 개발 범위 축소 |
| LLM | Claude 3.5 Sonnet | 코드 생성 품질, Prompt Caching, 비용 효율 |
| SDK | Anthropic SDK | Prompt Caching 지원 필수 |
| 모니터링 | Langfuse | LLM 호출 추적 및 비용 관리 |
| 인터페이스 | CLI | 자동화 파이프라인 통합 용이 |
| 패턴 | POM | 유지보수성 및 재사용성 |
| 도메인 지식 | 별도 문서 | 컨텍스트 제공 및 정확도 향상 |
| Mocking | 페이지별 설정 | 재사용성, 유연성, 명확한 의존성 |
| 스크린샷 | PC/Mobile 자동 생성 | 시각적 문서화, 다양한 상태 캡처 |

## 기술 스택

### 핵심 라이브러리
- **TypeScript**: 타입 안전성
- **Node.js**: 런타임
- **Anthropic SDK**: Claude API
- **Langfuse**: LLM 추적
- **MCP SDK**: MCP 클라이언트
- **Playwright MCP Server**: 브라우저 자동화
- **Commander.js**: CLI
- **Inquirer.js**: 대화형 프롬프트
- **Zod**: 스키마 검증

## 예상 사용 흐름

```bash
# 1. 초기 설정
e2e-agent init

# 2. 도메인 지식 문서 작성 (선택)
# docs/e2e-domain-knowledge.md

# 3. 테스트 생성
e2e-agent generate --scenario "로그인 테스트"

# Agent 실행:
# - 도메인 문서 로드
# - 시나리오 분석
# - 페이지 식별: LoginPage, MainPage
# - 경로 확인: /login, /
# - 브라우저 실행 (MCP)
# - 선택자 검증 및 사용자 확인
# - 페이지 객체 생성: tests/pages/LoginPage.ts
# - 테스트 파일 생성: tests/login.spec.ts
```

## 향후 개선 방향

1. **멀티 LLM 지원**: 추상화 레이어로 GPT-4, Gemini 지원
2. **멀티 브라우저**: Firefox, WebKit 지원
3. **병렬 처리**: 여러 시나리오 동시 생성
4. **테스트 리팩토링**: 기존 테스트 개선 제안
5. **시각적 회귀 테스트**: 스크린샷 비교
6. **CI/CD 통합**: GitHub Actions 템플릿
7. **웹 UI**: 브라우저 기반 인터페이스

## 결론

이 프로젝트는 AI와 브라우저 자동화를 결합하여 E2E 테스트 작성의 생산성을 크게 향상시킵니다. MCP 통합으로 개발 범위를 줄이고, Claude의 Prompt Caching으로 비용을 절감하며, 단계별 사용자 확인으로 정확도를 보장합니다.

핵심은 **"LLM이 추측하고, 브라우저로 검증하고, 사용자가 확인하는"** 협업 프로세스입니다.


## 6단계: 사용자 시나리오 검증 및 인터페이스 재설계

### 사용자 시나리오 작성

설계가 완료된 후, 실제 사용 시나리오를 작성하여 설계를 검증했습니다. 4가지 주요 시나리오를 작성:

1. 새 프로젝트에서 로그인 테스트 생성
2. 기존 프로젝트에 새 테스트 추가
3. 선택자 수정 및 재생성
4. 스크린샷만 재생성

### 중요한 발견: CLI 명령어의 한계

**문제 발견:**
초기 설계에서는 CLI 명령어 방식을 사용했습니다:
```bash
e2e-agent generate --scenario "로그인 테스트"
e2e-agent screenshot --page LoginPage --devices pc,mobile
e2e-agent update --page LoginPage
```

이 방식의 문제점:
- 명령어와 옵션을 외워야 함
- 복잡한 작업은 여러 플래그 필요
- 사용자 친화적이지 않음
- Agent의 강점(자연어 이해)을 활용하지 못함

### 해결책: 자연어 대화형 인터페이스

**새로운 접근:**
```
사용자: 로그인 테스트 만들어줘
사용자: @LoginPage의 pc, mobile 스크린샷 찍어줘
사용자: 상품 목록 페이지 테스트 추가해줘
```

**설계 논의 과정:**

1. **Agent 실행 방식**
   - 결정: CLI로 시작 (`e2e-agent`) → 대화형 모드 진입
   - 한 세션에서 여러 작업 연속 수행
   - `/clear` 명령어로 세션 초기화

2. **자연어 명령 스타일**
   - 결정: 간결한 자연어 ("로그인 테스트 만들어줘")
   - 필요시 Agent가 추가 정보 질문
   - 상세한 명령도 지원

3. **파일 참조 방식**
   - 결정: `@PageName` 문법 사용
   - 자동완성 지원 (다른 Agent처럼)
   - 예: `@LoginPage`, `@ProductListPage`

4. **확인 및 피드백 패턴**
   - Yes/No 질문: `1. yes 2. no 3. tell differently`
   - 에러 해결: `1. approve 2. tell differently`
   - 자연어 응답도 지원 ("응", "아니야" 등)

5. **MCP 도구 호출**
   - 스크린샷: 사용자가 명시적으로 요청할 때만
   - 여러 작업 요청 시 각 단계마다 확인

6. **에러 처리**
   - Agent가 해결 방법 제안
   - 사용자가 승인 또는 다른 방법 제시
   - `tell differently` 선택 시 자연어 입력 받음

### 설계 개선 사항

**추가된 기능:**
- 대화형 세션 관리
- 자연어 명령 파싱
- 파일 자동완성
- 구조화된 확인 패턴
- 유연한 에러 처리

**제거된 복잡성:**
- 복잡한 CLI 플래그
- 명령어 암기 필요성
- 엄격한 명령 구조

### 배운 점

1. **사용자 시나리오가 설계 검증의 핵심**
   - 실제 사용 흐름을 작성하면서 문제점 발견
   - CLI 명령어가 사용자 친화적이지 않음을 깨달음

2. **Agent의 강점 활용**
   - LLM의 자연어 이해 능력을 최대한 활용
   - 명령어 대신 대화로 상호작용
   - 사용자 의도 파악 및 추가 정보 질문

3. **유연성과 구조의 균형**
   - 자연어는 유연하지만 구조화된 확인도 필요
   - 선택지 제공으로 명확성 확보
   - 자연어 입력도 허용하여 유연성 유지

4. **점진적 개선의 중요성**
   - 초기 설계 → 시나리오 작성 → 문제 발견 → 재설계
   - 실제 사용을 상상하며 설계 검증
   - 사용자와의 대화를 통해 요구사항 명확화

## 최종 결론

### 핵심 설계 원칙

**"LLM이 추측하고, 브라우저로 검증하고, 사용자가 확인하는"** 협업 프로세스를 **자연어 대화**로 수행합니다.

사용자는 복잡한 명령어를 외울 필요 없이, 자연스러운 대화로 E2E 테스트를 생성하고 관리할 수 있습니다.

### 최종 설계 특징

✅ 자연어 대화형 인터페이스
✅ MCP 통합으로 브라우저 제어
✅ 자동 로그인 감지 및 처리
✅ 환경변수 기반 인증 관리
✅ Mocking 공통화 및 재사용
✅ 단계별 사용자 확인
✅ 도메인 지식 활용
✅ Prompt Caching으로 비용 절감


## 7단계: 구현 계획 수립 (MVP 우선 접근)

### 테스트 가능성과 점진적 개발의 중요성

설계가 완료된 후, 실제 구현을 위한 태스크를 작성할 때 중요한 깨달음:

**문제 인식:**
- 한 번에 모든 기능을 구현하면 검증이 어려움
- 어디서 문제가 생겼는지 파악하기 힘듦
- 테스트 없이 진행하면 나중에 디버깅 지옥

**해결 방안: MVP 우선 + 테스트 주도**

### Phase 기반 구현 전략

**Phase 1: 핵심 MVP**
- 목표: LLM + 기본 파일 생성만으로 동작하는 최소 버전
- 범위: CLI, LLM 통합, 간단한 페이지 객체 생성
- 검증: 파일이 생성되고 기본 구조가 맞는지 확인

**Phase 2: MCP 통합**
- 목표: 실제 브라우저로 선택자 자동 탐지
- 범위: MCP 클라이언트, 선택자 분석, 검증
- 검증: 생성된 테스트가 실제로 실행되는지 확인

**Phase 3: 고급 기능**
- 목표: 인증, Mocking, 스크린샷 등 부가 기능
- 범위: 자동 로그인, Mocking 생성, 스크린샷 MCP
- 검증: 각 기능이 독립적으로 작동하는지 확인

**Phase 4: 배포 준비**
- 목표: 문서화 및 패키징
- 범위: README, 예제, NPM 패키지
- 검증: 처음 사용하는 사람도 설치 가능한지 확인

### 각 태스크별 테스트 전략

**3가지 테스트 레벨:**

1. **단위 테스트 (Vitest)**
   ```typescript
   test('설정 파일 로드', () => {
     const config = loadConfig('.e2e-agent.config.json');
     expect(config.baseUrl).toBeDefined();
   });
   ```
   - 각 함수/클래스 메서드 테스트
   - Mock을 사용한 격리 테스트
   - 빠른 피드백

2. **통합 테스트**
   ```typescript
   test('LLM 호출', async () => {
     const response = await llm.chat([...]);
     expect(response.content).toBeDefined();
   });
   ```
   - 여러 컴포넌트 함께 테스트
   - 실제 외부 서비스 호출 (LLM, MCP)
   - 실제 동작 검증

3. **수동 E2E 테스트**
   ```bash
   npm start
   > 로그인 테스트 만들어줘
   # 파일 생성 확인
   # 생성된 테스트 실행
   ```
   - 실제 사용자 시나리오
   - CLI 인터페이스 검증
   - 전체 워크플로우 확인

### 각 Phase별 완료 기준

**명확한 체크리스트:**
- Phase 1: CLI 실행 → 파일 생성 → 기본 구조 확인
- Phase 2: 브라우저 분석 → 선택자 탐지 → 테스트 실행 성공
- Phase 3: 자동 로그인 → Mocking 생성 → 스크린샷 캡처
- Phase 4: 문서 완성 → 패키지 설치 → 예제 실행

### 배운 점

1. **작은 단위로 검증하며 진행**
   - 한 번에 많이 구현하지 않기
   - 각 단계마다 테스트로 검증
   - 문제 발생 시 범위가 좁아 디버깅 쉬움

2. **테스트 가이드의 중요성**
   - "어떻게 테스트할까?"를 미리 정의
   - 구현 중 방향을 잃지 않음
   - 완료 기준이 명확함

3. **MVP 우선 접근의 가치**
   - 핵심 기능부터 완성
   - 빠르게 전체 흐름 검증
   - 이후 기능 추가는 선택적

4. **Phase별 목표 설정**
   - 각 Phase가 독립적으로 가치 제공
   - Phase 1만 완성해도 사용 가능
   - 점진적 개선 가능

### 구현 시작 준비 완료

이제 다음 순서로 진행:
1. Phase 1 태스크 시작
2. 각 태스크마다 테스트 작성 및 실행
3. 완료 기준 충족 확인
4. 다음 Phase로 진행

**핵심 원칙: "작게 만들고, 테스트하고, 확인하고, 다음으로"**
